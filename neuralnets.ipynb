{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332c6796",
   "metadata": {},
   "source": [
    "Neural Network is a computer model which contains many neurons segregated as layers , there are mainly 3 layers , input output and hidden layers . A neuron is a small computational unit which is connected to other neurons and takes input from them , it has a bias , weights and an activation function . The input in the neuron comes in the form of summation of product of input values and weights and at last bias of the current neuron is added to it , later this value is passed through an activation function in which it decides the final value to output to next neuron or even the decision to whether output the value to the next neuron or not                                                                             \n",
    "neuron_output = Activationfunction((Input X Weight) + Bias)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df6a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = []\n",
    "input_layer=[1,2,3,3.5]\n",
    "biases=[2,3,.5]\n",
    "weights=[[.5,.5,.4,.6],\n",
    "         [.43,.52,.543,.243],\n",
    "         [.43 ,.54, .75 ,.84]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7128bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.800000000000001, 6.9495000000000005, 7.199999999999999]\n"
     ]
    }
   ],
   "source": [
    "for neuron_weights , bias in zip(weights,biases):\n",
    "    neuron_output = 0\n",
    "    for inputs,nweights in zip(input_layer,neuron_weights):\n",
    "        neuron_output+=inputs*nweights\n",
    "    neuron_output+=bias\n",
    "    output_layer.append(neuron_output)  \n",
    "print(output_layer)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e94881",
   "metadata": {},
   "source": [
    "Instead of using loops to multiply weights and inputs we can use dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847c06b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(6.800000000000001), np.float64(6.9495000000000005), np.float64(7.199999999999999)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "output_layer=[]\n",
    "for neuron_weight,bias in zip(weights,biases):\n",
    "    neuron_output = np.dot(neuron_weight,input_layer) + bias\n",
    "    output_layer.append(neuron_output)    \n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3e0f9",
   "metadata": {},
   "source": [
    "Batch size mean the total inputs used to train a neural network at once , for example a photo with 28x28  dimensions has 784 grids as input features , if we input the model with only one image than the batch size is one , the neural network does not train properly with it , so if we input 32 image at once that is batch size of 32 it trains the neural network on 32 images at once ,not iteratively but through matrix multiplicatoin and vectors all the 32 inputs are processed at once unlike traditional ml where the model is trained by going through inputs iteratively , also we cannot use all the images at once because that would cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2f000",
   "metadata": {},
   "source": [
    "So now the input changes from 1d to 2d with the number of rows being the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f451fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=[[1,2,3,3.5],\n",
    "             [3.2, 5.3,3.4,5.1],\n",
    "             [2.3,2.4,4.3,4.2],\n",
    "             [3.3,4.3,2.4,5.3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459bea1",
   "metadata": {},
   "source": [
    "In the weights 2d array each row represent the weights of individual neuron but during matrix multiplication to get the output values of neurons we need each columns to be the weights of individual neuron so use transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815b3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.8     6.9495  7.2   ]\n",
      " [10.67   10.2175 11.572 ]\n",
      " [ 8.59    8.5925  9.538 ]\n",
      " [ 9.94    9.2461 10.493 ]]\n"
     ]
    }
   ],
   "source": [
    "output_layer=[]\n",
    "weights = np.transpose(weights)\n",
    "output_layer = np.dot(input_layer,weights)  + biases\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1460ca",
   "metadata": {},
   "source": [
    "In this output above matrix each row represents the output neurons or the output layers of one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765244f5",
   "metadata": {},
   "source": [
    "Now we add one more layer and the weights and biases keep on incresasing like shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9a462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer1=[[1,2,3,3.5],\n",
    "             [3.2, 5.3,3.4,5.1],\n",
    "             [2.3,2.4,4.3,4.2],\n",
    "             [3.3,4.3,2.4,5.3]]\n",
    "\n",
    "biases1=[2,3,.5]\n",
    "weights1=[[.5,.5,.4,.6],\n",
    "         [.43,.52,.543,.243],\n",
    "         [.43 ,.54, .75 ,.84]]\n",
    "\n",
    "biases2=[2,3,3.8,1.5]\n",
    "weights2=[[.15,.5,.74,.86],\n",
    "         [.3,.52,.6543,.243],\n",
    "         [.243 ,.454, .735 ,.384]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda21e0",
   "metadata": {},
   "source": [
    "we try to keep the value ranges of input weights or any other parameters between -1 and 1 mostly because if the values are not in that range after passing through multiple layers,neurons the value may be very big and problem occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c460fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "X=[[1,2,3,3.5],\n",
    "  [3.2, 5.3,3.4,5.1],\n",
    "  [2.3,2.4,4.3,4.2],\n",
    "  [3.3,4.3,2.4,5.3]]\n",
    "\n",
    "class Layer_dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55347800",
   "metadata": {},
   "source": [
    "Now we can keep on creating layer objects but the input of the next layer must be same as the output of the previous layer for obious reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d31ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.40948745 11.89243132  2.24108282  3.4952175   1.0310096 ]\n",
      " [ 2.65688196 18.88030488  3.87098947  8.63413866  5.30560435]\n",
      " [ 3.73267301 15.72908197  4.29863757  6.74441603  3.60222958]\n",
      " [ 3.73325636 16.81477449  3.31815109  8.90238535  4.46707927]]\n"
     ]
    }
   ],
   "source": [
    "layer1=Layer_dense(4,5)\n",
    "layer2=Layer_dense(5,32)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aaea34",
   "metadata": {},
   "source": [
    "ACTIVATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492db1aa",
   "metadata": {},
   "source": [
    "It decides whether to fire the neuron or stay quiet , in other ways whether to allow the processed value in the neuron to pass to the next neuron as input or not , functions like RELU (max(x,0)) , step wise function and sigmoid functions are used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffb466",
   "metadata": {},
   "source": [
    "WHY IS ACTIVATION FUNCTION REQUIRED?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d5819",
   "metadata": {},
   "source": [
    "if we dont use activation function and just pass the inputXweight + bias to the next neuron then all the neurons will do the same thing so the final output will be a linear funcion like Wx + B so it can only predict values linerly and cannot adapt according to the data required , so to overcome this in between we need to introduce non linearity and activation functions do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c2ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # this block is just to create the dataset\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def create_data(points, classes):\n",
    "    X = np.zeros((points * classes, 2))\n",
    "    Y = np.zeros(points * classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points * class_number, points * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, points) + np.random.randn(points) * 0.2\n",
    "        X[ix] = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]\n",
    "        Y[ix] = class_number\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3070c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "X,Y = create_data(100,3) \n",
    "\n",
    "\n",
    "class Layer_dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = 0.10*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_Relu:\n",
    "    def forward(self,layer_output):\n",
    "        self.activated_output = np.maximum(0,layer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b01248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer_dense(2,5)   # since the number of data features is 2 (coordinates)\n",
    "activation1 = Activation_Relu()\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer1output = activation1.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8673eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.00026099 0.         0.         0.0019636 ]\n",
      " [0.         0.00326282 0.00103883 0.         0.00124191]\n",
      " ...\n",
      " [0.         0.         0.         0.02708142 0.1919911 ]\n",
      " [0.00816536 0.         0.         0.09704872 0.12448636]\n",
      " [0.00424775 0.         0.         0.08921545 0.14418652]]\n"
     ]
    }
   ],
   "source": [
    "print(layer1output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
